{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Passanger Survival ","metadata":{}},{"cell_type":"markdown","source":"# import necessary libraries","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np                 \nimport scipy.stats                \nimport csv                         \nimport pandas as pd                \nimport matplotlib.pyplot as plt    \nimport math\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.059472Z","iopub.execute_input":"2024-07-05T02:27:07.059880Z","iopub.status.idle":"2024-07-05T02:27:07.067788Z","shell.execute_reply.started":"2024-07-05T02:27:07.059850Z","shell.execute_reply":"2024-07-05T02:27:07.066250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Collection and Data Analysis","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\", index_col='PassengerId')\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntrain_data.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.080826Z","iopub.execute_input":"2024-07-05T02:27:07.081273Z","iopub.status.idle":"2024-07-05T02:27:07.124931Z","shell.execute_reply.started":"2024-07-05T02:27:07.081238Z","shell.execute_reply":"2024-07-05T02:27:07.123522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.127408Z","iopub.execute_input":"2024-07-05T02:27:07.127875Z","iopub.status.idle":"2024-07-05T02:27:07.154615Z","shell.execute_reply.started":"2024-07-05T02:27:07.127831Z","shell.execute_reply":"2024-07-05T02:27:07.153251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can begin by removing certain features that don't contribute to the relationships we are trying to establish. For example, the TicketId is unique to each passenger and doesn't provide any meaningful insights. Although we could categorize ticket values by their last digit and assign each passenger to a bucket ranging from 0 to 9, for this report, we will ignore the TicketId.","metadata":{}},{"cell_type":"code","source":"train_data.describe(include=\"all\") ","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.156376Z","iopub.execute_input":"2024-07-05T02:27:07.156833Z","iopub.status.idle":"2024-07-05T02:27:07.209516Z","shell.execute_reply.started":"2024-07-05T02:27:07.156792Z","shell.execute_reply":"2024-07-05T02:27:07.208207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.describe(include=\"all\")  ","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.212396Z","iopub.execute_input":"2024-07-05T02:27:07.212786Z","iopub.status.idle":"2024-07-05T02:27:07.265690Z","shell.execute_reply.started":"2024-07-05T02:27:07.212752Z","shell.execute_reply":"2024-07-05T02:27:07.264215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the chart above, we can gather several insights about the data. Firstly, the training dataset shows missing entries for the ages of passengers, which need to be addressed by filling these gaps to prevent skewness. Similarly, the test dataset has missing values for Age and one missing value for Fare. These issues must be resolved before processing the data.\n\nSecondly, the chart provides statistical tools used on each feature. For instance, the average survival rate in the training dataset is approximately 38.38%, with a high standard deviation of about 0.4865. The mean of Pclass (1, 2, 3) is around 2.265, indicating a higher proportion of passengers in the second and third classes. This will help us identify which features are crucial for our analysis.\n\nAdditionally, the mean age is around 29.69 with a standard deviation of approximately 14.18, showing significant deviation from the mean. SibSp, representing the number of siblings or spouses aboard, has a mean of about 0.44, while Parch, indicating the number of parents or children aboard, is also noteworthy. The Fare feature, representing the amount paid by passengers, has a mean of around 35.627. Other features are non-numerical. Furthermore, there are null values in the Age, Cabin, and Embarked sections, which need to be addressed.","metadata":{}},{"cell_type":"code","source":"print(train_data.dtypes)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.267540Z","iopub.execute_input":"2024-07-05T02:27:07.267999Z","iopub.status.idle":"2024-07-05T02:27:07.276231Z","shell.execute_reply.started":"2024-07-05T02:27:07.267956Z","shell.execute_reply":"2024-07-05T02:27:07.274959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe relationship between the embarkation point, class, and survival provides valuable insights into patterns that might exist among these variables. By analyzing this plot, we can identify any correlations between where passengers boarded, their class, and their survival outcomes.","metadata":{}},{"cell_type":"markdown","source":"# Testing Feature relation to Survival rates:","metadata":{}},{"cell_type":"markdown","source":"During this process, we will first identify the features that significantly impact the survival rate of the passengers. Then, we will clean the dataset to retain only these relevant features for further analysis","metadata":{}},{"cell_type":"markdown","source":"# Did the class of the passengers effect their survival rate\n","metadata":{}},{"cell_type":"code","source":"#Visualization: \npivot_class_survived = train_data.pivot_table(index='Pclass', columns='Survived', aggfunc='size', fill_value=0)\n\n#Survival rate by class\npivot_class_survived['Survival Rate'] = pivot_class_survived[1] / (pivot_class_survived[0] + pivot_class_survived[1])\nprint(pivot_class_survived)\n\n# Plotting the survival rate by class\npivot_class_survived['Survival Rate'].plot(kind='bar', color='skyblue')\nplt.ylabel('Survival Rate')\nplt.title('Survival Rate by Passenger Class')\nplt.show()\n\nchi2, p, dof, ex = chi2_contingency(pivot_class_survived[[0, 1]])\nprint(f\"Chi-Square Statistic: {chi2}, p-value: {p}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.278658Z","iopub.execute_input":"2024-07-05T02:27:07.279035Z","iopub.status.idle":"2024-07-05T02:27:07.582572Z","shell.execute_reply.started":"2024-07-05T02:27:07.279003Z","shell.execute_reply":"2024-07-05T02:27:07.581200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe above observation is significant as it indicates that passengers in higher classes had a better chance of survival compared to those in lower classes. The p-value is less than 0.05, which is the threshold for statistical significance, indicating a strong relationship. Additionally, the high chi-square value suggests a significant deviation from the null hypothesis, which posits that there is no relationship between class and survival rate.","metadata":{}},{"cell_type":"markdown","source":"**Does sex influence the survival rate? Given that P-class impacts survival rates, we can further analyze each class by separating passengers based on gender and calculating their respective survival rates**","metadata":{}},{"cell_type":"code","source":"train_data[['Survived', 'Sex', 'Pclass']].groupby(['Pclass', 'Sex']).mean()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.583963Z","iopub.execute_input":"2024-07-05T02:27:07.584358Z","iopub.status.idle":"2024-07-05T02:27:07.605090Z","shell.execute_reply.started":"2024-07-05T02:27:07.584324Z","shell.execute_reply":"2024-07-05T02:27:07.603244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The averages for each class reveal that upper-class females had the highest survival rate. Additionally, the data indicates that females, in general, had a higher survival rate compared to their male counterparts.","metadata":{}},{"cell_type":"code","source":"contingency_table = pd.crosstab(train_data['Sex'], train_data['Survived'])\n\n# Perform the Chi-Square Test\nchi2, p, dof, expected = chi2_contingency(contingency_table)\n\n# Output the results\nprint(f\"Chi-Square Statistic: {chi2}\")\nprint(f\"p-value: {p}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.607003Z","iopub.execute_input":"2024-07-05T02:27:07.607809Z","iopub.status.idle":"2024-07-05T02:27:07.626173Z","shell.execute_reply.started":"2024-07-05T02:27:07.607767Z","shell.execute_reply":"2024-07-05T02:27:07.624813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an even higher Chi-square value and a lower p-value, indicating a stronger correlation with the survival rate compared to the feature Pclass.","metadata":{}},{"cell_type":"code","source":"grouped_data = train_data[['Survived', 'Sex', 'Pclass']].groupby(['Pclass', 'Sex']).mean().reset_index()\n\n# Bar graph to visualize the relation\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Pclass', y='Survived', hue='Sex', data=grouped_data)\n\nplt.title('Survival Rate by Passenger Class and Sex')\nplt.xlabel('Passenger Class')\nplt.ylabel('Survival Rate')\nplt.legend(title='Sex')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.630512Z","iopub.execute_input":"2024-07-05T02:27:07.630912Z","iopub.status.idle":"2024-07-05T02:27:07.929611Z","shell.execute_reply.started":"2024-07-05T02:27:07.630879Z","shell.execute_reply":"2024-07-05T02:27:07.928232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Does age contribute to the survival rates?","metadata":{}},{"cell_type":"markdown","source":"\nBefore visualizing how each age group contributes to survival rates, we have the data ready for analysis.","metadata":{}},{"cell_type":"code","source":"# Assuming 'train_data' is your DataFrame and it includes 'Survived', 'Sex', and 'Age'\nplt.figure(figsize=(10, 10))\n\n# Create a FacetGrid with normalized histograms\ng = sns.FacetGrid(train_data, row='Sex', col='Survived')\ng.map(plt.hist, 'Age', bins=30, density=True)\n\n# Adding labels and titles for clarity\ng.set_xlabels('Age')\ng.set_ylabels('Density')\ng.set_titles('Sex: {row_name}, Survived: {col_name}')\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:07.931107Z","iopub.execute_input":"2024-07-05T02:27:07.931469Z","iopub.status.idle":"2024-07-05T02:27:09.434675Z","shell.execute_reply.started":"2024-07-05T02:27:07.931439Z","shell.execute_reply":"2024-07-05T02:27:09.433394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_bins = [0, 18, 30, 40, 50, 60, 120]  # Adjust the bins as you see fit\nage_labels = ['0-18', '19-30', '31-40', '41-50', '51-60', '60+']\n\n# Create a new column for the age groups\ntrain_data['AgeGroup'] = pd.cut(train_data['Age'], bins=age_bins, labels=age_labels, right=False)\n\n# Create the contingency table\ncontingency_table = pd.crosstab(train_data['AgeGroup'], train_data['Survived'])\n\n# Apply the Chi-Square test\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# Output the results\nprint(f\"Chi-Square Statistic: {chi2}\")\nprint(f\"p-value: {p_value}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.436800Z","iopub.execute_input":"2024-07-05T02:27:09.437262Z","iopub.status.idle":"2024-07-05T02:27:09.461299Z","shell.execute_reply.started":"2024-07-05T02:27:09.437217Z","shell.execute_reply":"2024-07-05T02:27:09.460046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_females = train_data['Sex'].value_counts()['female']\nnum_males = train_data['Sex'].value_counts()['male']\n\nprint(f\"Number of females: {num_females}\")\nprint(f\"Number of males: {num_males}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.463056Z","iopub.execute_input":"2024-07-05T02:27:09.463538Z","iopub.status.idle":"2024-07-05T02:27:09.472724Z","shell.execute_reply.started":"2024-07-05T02:27:09.463506Z","shell.execute_reply":"2024-07-05T02:27:09.471355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above observations, it is evident that there is a relationship between age and survival, but it appears to be less significant. When ages are categorized into buckets and subjected to chi-square analysis with associated p-values, the results indicate a correlation between age and survival rate. However, this relationship is not as pronounced as the other features we have identified thus far.","metadata":{}},{"cell_type":"markdown","source":"# Do passengers having siblings or a spouse influence their survival rate?","metadata":{}},{"cell_type":"code","source":"contingency_table = pd.crosstab(train_data['SibSp'], train_data['Survived'])\n\n# Perform the Chi-Square Test\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# Output the results\nprint(f\"Chi-Square Statistic: {chi2}\")\nprint(f\"p-value: {p_value}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.474198Z","iopub.execute_input":"2024-07-05T02:27:09.474559Z","iopub.status.idle":"2024-07-05T02:27:09.494670Z","shell.execute_reply.started":"2024-07-05T02:27:09.474529Z","shell.execute_reply":"2024-07-05T02:27:09.493122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Does the presence of parents influence the survival rate?","metadata":{}},{"cell_type":"code","source":"contingency_table = pd.crosstab(train_data['Parch'], train_data['Survived'])\n\n# Perform the Chi-Square Test\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# Output the results\nprint(f\"Chi-Square Statistic: {chi2}\")\nprint(f\"p-value: {p_value}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.496090Z","iopub.execute_input":"2024-07-05T02:27:09.496508Z","iopub.status.idle":"2024-07-05T02:27:09.513717Z","shell.execute_reply.started":"2024-07-05T02:27:09.496474Z","shell.execute_reply":"2024-07-05T02:27:09.512326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fare Rate influence on Survival rates","metadata":{}},{"cell_type":"code","source":"# Define fare bins and labels\nfare_bins = [0, 25, 50, 100, 200, np.inf]\nfare_labels = ['0-25', '26-50', '51-100', '101-200', '200+']\ntrain_data['FareBin'] = pd.cut(train_data['Fare'], bins=fare_bins, labels=fare_labels)\n\n# Create the contingency table\ncontingency_table_fare = pd.crosstab(train_data['FareBin'], train_data['Survived'])\n\n# Perform the Chi-Square Test\nchi2_fare, p_value_fare, dof_fare, expected_fare = chi2_contingency(contingency_table_fare)\nprint(f\"Chi-Square Statistic for Fare: {chi2_fare}\")\nprint(f\"p-value for Fare: {p_value_fare}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.515453Z","iopub.execute_input":"2024-07-05T02:27:09.515939Z","iopub.status.idle":"2024-07-05T02:27:09.540579Z","shell.execute_reply.started":"2024-07-05T02:27:09.515903Z","shell.execute_reply":"2024-07-05T02:27:09.539240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"survival_rate_by_fare = (contingency_table_fare[1] / contingency_table_fare.sum(axis=1)) * 100\n\n# Create a bar plot for visualization\nplt.figure(figsize=(10, 6))\nsns.barplot(x=survival_rate_by_fare.index, y=survival_rate_by_fare.values)\nplt.title('Survival Rate by Fare Bins')\nplt.xlabel('Fare Bins')\nplt.ylabel('Survival Rate (%)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.542106Z","iopub.execute_input":"2024-07-05T02:27:09.542495Z","iopub.status.idle":"2024-07-05T02:27:09.866925Z","shell.execute_reply.started":"2024-07-05T02:27:09.542465Z","shell.execute_reply":"2024-07-05T02:27:09.865623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Des where a passenger Embarked influence their Survival Rate","metadata":{}},{"cell_type":"code","source":"contingency_table_embarked = pd.crosstab(train_data['Embarked'], train_data['Survived'])\n\n# Perform the Chi-Square Test\nchi2_embarked, p_value_embarked, dof_embarked, expected_embarked = chi2_contingency(contingency_table_embarked)\n\n# Output the results\nprint(f\"Chi-Square Statistic for Embarked: {chi2_embarked}\")\nprint(f\"p-value for Embarked: {p_value_embarked}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.868328Z","iopub.execute_input":"2024-07-05T02:27:09.868702Z","iopub.status.idle":"2024-07-05T02:27:09.892844Z","shell.execute_reply.started":"2024-07-05T02:27:09.868671Z","shell.execute_reply":"2024-07-05T02:27:09.891557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.countplot(x='Embarked', hue='Pclass', data=train_data)\n\nplt.title('Passenger Count by Embarkation Point and Class')\nplt.xlabel('Embarkation Point')\nplt.ylabel('Count')\nplt.legend(title='Passenger Class', loc='upper right', labels=['1st Class', '2nd Class', '3rd Class'])\nplt.show()\n\nplt.figure(figsize=(12, 8))\nsns.countplot(x='Embarked', hue='Survived', data=train_data)\n\nplt.title('Survival Rate by Embarkation Point')\nplt.xlabel('Embarkation Point')\nplt.ylabel('Count')\nplt.legend(title='Survived', loc='upper right', labels=['No', 'Yes'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:09.894656Z","iopub.execute_input":"2024-07-05T02:27:09.895923Z","iopub.status.idle":"2024-07-05T02:27:10.654364Z","shell.execute_reply.started":"2024-07-05T02:27:09.895873Z","shell.execute_reply":"2024-07-05T02:27:10.653121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning Checklist:\n# To-do List:\n# \n# 1-Fill in missing values for Age.\n# 2-Prepare Embarkment data for insertion into models by categorizing it.\n# 3-Encode gender: Male as 1 and Female as 0.","metadata":{}},{"cell_type":"code","source":"#Clean the data:\ntrain_data['Sex'] = train_data['Sex'].map({'female': 0, 'male': 1}).astype(int)\ntrain_data['Age'].fillna(train_data['Age'].median(), inplace=True)\nembarked_dummies = pd.get_dummies(train_data['Embarked'], prefix='Embarked')\ntrain_data = pd.concat([train_data, embarked_dummies], axis=1)\ntrain_data = train_data.drop('Embarked', axis=1)\ntrain_data = train_data.drop(['Ticket', 'Name', 'Cabin'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:10.655894Z","iopub.execute_input":"2024-07-05T02:27:10.656392Z","iopub.status.idle":"2024-07-05T02:27:10.680083Z","shell.execute_reply.started":"2024-07-05T02:27:10.656356Z","shell.execute_reply":"2024-07-05T02:27:10.677386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe(include=\"all\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:10.681898Z","iopub.execute_input":"2024-07-05T02:27:10.682845Z","iopub.status.idle":"2024-07-05T02:27:10.734142Z","shell.execute_reply.started":"2024-07-05T02:27:10.682800Z","shell.execute_reply":"2024-07-05T02:27:10.732867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.dtypes)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:10.735603Z","iopub.execute_input":"2024-07-05T02:27:10.735945Z","iopub.status.idle":"2024-07-05T02:27:10.743815Z","shell.execute_reply.started":"2024-07-05T02:27:10.735913Z","shell.execute_reply":"2024-07-05T02:27:10.742371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we note the categorical types, which will require further classification before any processing can proceed.","metadata":{}},{"cell_type":"markdown","source":"# Correlation Matrix\nNow that we've analyzed and cleaned the data, let's gain a comprehensive overview of correlations using a heatmap:","metadata":{}},{"cell_type":"code","source":"numeric_cols = train_data.select_dtypes(include=[np.number, bool])\n\n# Calculate the correlation matrix\ncorrelation_matrix = numeric_cols.corr()\n\n\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=.5)\nplt.title('Correlation Matrix Heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:10.745872Z","iopub.execute_input":"2024-07-05T02:27:10.746336Z","iopub.status.idle":"2024-07-05T02:27:11.474615Z","shell.execute_reply.started":"2024-07-05T02:27:10.746299Z","shell.execute_reply":"2024-07-05T02:27:11.473238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# \n# Training\n# Using Python's built-in libraries.","metadata":{}},{"cell_type":"code","source":"features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\nX = train_data[features]\ny = train_data['Survived']\n\n# Split the data into new training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# Train your model on the new training data\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Validate the model on the new validation set\npredictions = model.predict(X_val)\naccuracy = accuracy_score(y_val, predictions)\nprint(f\"Validation Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:11.476726Z","iopub.execute_input":"2024-07-05T02:27:11.477184Z","iopub.status.idle":"2024-07-05T02:27:11.522270Z","shell.execute_reply.started":"2024-07-05T02:27:11.477130Z","shell.execute_reply":"2024-07-05T02:27:11.520770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our Implementation of Logistic Regression:\nWe use the sigmoid function to map values between [0, 1]. Maximizing the log-likelihood involves taking the gradient, which is computed as the dot product of X.T and the error (y - prediction or sigmoid(z)). After obtaining this gradient, we update and iterate through the process.","metadata":{}},{"cell_type":"code","source":"\ndef sigmoid(z):\n    # Ensure z is a NumPy array to handle operations correctly\n    z = np.array(z)\n    return 1 / (1 + np.exp(-z))\n\ndef log_likelihood(X, y, theta):\n    z = np.dot(X, theta)\n    predictions = sigmoid(z)\n    ll = np.sum(y * np.log(predictions + 1e-9) + (1 - y) * np.log(1 - predictions + 1e-9))\n    return ll\n\ndef compute_gradient(X, y, theta):\n    z = np.dot(X, theta)\n  \n    predictions = sigmoid(z)\n    gradient = np.dot(X.T, (y - predictions)) # dot product with error \n    return gradient\n\n\ndef gradient_descent(X, y, theta, learning_rate, num_iterations):\n    for i in range(num_iterations):\n        gradient = compute_gradient(X, y, theta)\n        theta += learning_rate * gradient\n    return theta\n\ndef predict(X, theta):\n    z = np.dot(X, theta)\n    probabilities = sigmoid(z)\n    return probabilities","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:11.529483Z","iopub.execute_input":"2024-07-05T02:27:11.529988Z","iopub.status.idle":"2024-07-05T02:27:11.543948Z","shell.execute_reply.started":"2024-07-05T02:27:11.529946Z","shell.execute_reply":"2024-07-05T02:27:11.542498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming train_data and setup are correctly defined as previously discussed\n\n# Feature names and dummy encoding\nfeature_names = [\n    'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n    'Embarked_C', 'Embarked_Q', 'Embarked_S',\n    'AgeGroup_19-30', 'AgeGroup_31-40', 'AgeGroup_41-50', 'AgeGroup_51-60', 'AgeGroup_60+',\n    'FareBin_26-50', 'FareBin_51-100', 'FareBin_101-200', 'FareBin_200+'\n]\n\ntrain_data = pd.get_dummies(train_data, columns=['AgeGroup', 'FareBin'], drop_first=True)\n\n# Prepare data\nX = train_data[feature_names].values\ny = train_data['Survived'].values\nX = np.hstack((np.ones((X.shape[0], 1)), X))  # Add intercept\nX = np.asarray(X, dtype=np.float64)\n\n# Split the data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# Train the model\ntheta = np.zeros((X_train.shape[1], 1))\nlearning_rate = 0.9\nnum_iterations = 8000\ntheta_final = gradient_descent(X_train, y_train.reshape(-1, 1), theta, learning_rate, num_iterations)\n\n# Predict and validate\nprobabilities = predict(X_val, theta_final)\npredictions = (probabilities > 0.5).astype(int)\naccuracy = accuracy_score(y_val, predictions)\nprint(f\"Validation Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:11.545915Z","iopub.execute_input":"2024-07-05T02:27:11.546380Z","iopub.status.idle":"2024-07-05T02:27:12.721637Z","shell.execute_reply.started":"2024-07-05T02:27:11.546344Z","shell.execute_reply":"2024-07-05T02:27:12.719988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Decision Tree\ntree_model = DecisionTreeClassifier(random_state=42)\ntree_model.fit(X_train, y_train)\n\n# Validate the model on the new validation set\ntree_predictions = tree_model.predict(X_val)\ntree_accuracy = accuracy_score(y_val, tree_predictions)\nprint(f\"Decision Tree Validation Accuracy Sk-learn: {tree_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:12.724568Z","iopub.execute_input":"2024-07-05T02:27:12.725782Z","iopub.status.idle":"2024-07-05T02:27:12.752510Z","shell.execute_reply.started":"2024-07-05T02:27:12.725712Z","shell.execute_reply":"2024-07-05T02:27:12.750958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree from scratch:","metadata":{}},{"cell_type":"code","source":"#implement dicision tree\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Load datasets\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n#preproccessing \n# Fill missing values\ntrain_data['Age'].fillna(train_data['Age'].median(), inplace=True)\ntrain_data['Embarked'].fillna('S', inplace=True)#filling S because S is the majority, it has least amout of impact on our accuracy\n\n# Convert categorical variables to numeric\ntrain_data['Sex'] = train_data['Sex'].map({'male': 0, 'female': 1})\ntrain_data = pd.get_dummies(train_data, columns=['Embarked'])   #created three columns C Q S\n#alternative, using convert them into numerical values\n#train_data['Embarked'] = train_data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n\n\n# Assuming 'train_data' is loaded and prepared\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C', 'Embarked_Q','Embarked_S']\nX = train_data[features]\ny = train_data['Survived']\n\n# Split the data into new training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:12.755580Z","iopub.execute_input":"2024-07-05T02:27:12.756837Z","iopub.status.idle":"2024-07-05T02:27:12.810505Z","shell.execute_reply.started":"2024-07-05T02:27:12.756773Z","shell.execute_reply":"2024-07-05T02:27:12.809232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nclass Node:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\n    def is_leaf_node(self):\n        return self.value is not None\n\ndef calculate_entropy(y):\n    proportions = np.bincount(y) / len(y)\n    return -np.sum([p * np.log2(p) for p in proportions if p > 0])\n\ndef best_split(X, y, num_features):\n    best_gain = -1\n    split_idx, split_threshold = None, None\n    for feature_idx in range(num_features):\n        thresholds, classes = zip(*sorted(zip(X[:, feature_idx], y)))\n        unique_thresholds = np.unique(thresholds)  # Ensure unique thresholds for comparison\n        for i in range(1, len(unique_thresholds)):\n            threshold = (unique_thresholds[i] + unique_thresholds[i - 1]) / 2\n            lhs = y[X[:, feature_idx] <= threshold]\n            rhs = y[X[:, feature_idx] > threshold]\n            gain = information_gain(y, lhs, rhs)\n            if gain > best_gain:\n                best_gain = gain\n                split_idx = feature_idx\n                split_threshold = threshold\n    return split_idx, split_threshold\n\ndef information_gain(y, lhs, rhs):\n    parent_entropy = calculate_entropy(y)\n    n = len(y)\n    l, r = len(lhs), len(rhs)\n    if l == 0 or r == 0:\n        return 0\n    child_entropy = (l / n) * calculate_entropy(lhs) + (r / n) * calculate_entropy(rhs)\n    return parent_entropy - child_entropy\n\ndef build_tree(X, y, depth=0, max_depth=10):\n    num_samples, num_features = X.shape\n    num_labels = len(np.unique(y))\n    if depth >= max_depth or num_labels == 1:\n        leaf_value = Counter(y).most_common(1)[0][0]\n        return Node(value=leaf_value)\n    split_idx, threshold = best_split(X, y, num_features)\n    if split_idx is None:\n        return Node(value=Counter(y).most_common(1)[0][0])\n    left_idxs = X[:, split_idx] <= threshold\n    right_idxs = X[:, split_idx] > threshold\n    left = build_tree(X[left_idxs], y[left_idxs], depth + 1, max_depth)\n    right = build_tree(X[right_idxs], y[right_idxs], depth + 1, max_depth)\n    return Node(split_idx, threshold, left, right)\n\ndef predict(node, x):\n    while not node.is_leaf_node():\n        if x[node.feature] <= node.threshold:\n            node = node.left\n        else:\n            node = node.right\n    return node.value\n\ndef decision_tree_predictions(X, tree):\n    return np.array([predict(tree, xi) for xi in X])\n\nX_train_np = X_train.to_numpy()\ny_train_np = y_train.to_numpy()\nX_val_np = X_val.to_numpy()\n\n# Build the tree\ntree = build_tree(X_train_np, y_train_np, max_depth=3)\n\n# Make predictions\npredictions = decision_tree_predictions(X_val_np, tree)\naccuracy = accuracy_score(y_val, predictions)\nprint(f\"decision tree implementation Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:12.812682Z","iopub.execute_input":"2024-07-05T02:27:12.813500Z","iopub.status.idle":"2024-07-05T02:27:13.065584Z","shell.execute_reply.started":"2024-07-05T02:27:12.813454Z","shell.execute_reply":"2024-07-05T02:27:13.064142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# using Random Forest\nforest_model = RandomForestClassifier(random_state=42)\nforest_model.fit(X_train, y_train)\n\n# Validate the model on the new validation set\nforest_predictions = forest_model.predict(X_val)\nforest_accuracy = accuracy_score(y_val, forest_predictions)\nprint(f\"Random Forest Validation Accuracy: {forest_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:13.067350Z","iopub.execute_input":"2024-07-05T02:27:13.067819Z","iopub.status.idle":"2024-07-05T02:27:13.396859Z","shell.execute_reply.started":"2024-07-05T02:27:13.067777Z","shell.execute_reply":"2024-07-05T02:27:13.395423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# using Linear Regression\nlin_model = LinearRegression()\nlin_model.fit(X_train, y_train)\n\n# Predicting with the linear regression model\nlin_predictions = lin_model.predict(X_val)\n\n# Thresholding the predictions to get binary outcome\nlin_predictions_binary = np.where(lin_predictions > 0.5, 1, 0)\n\n# Validate the model on the new validation set\nlin_accuracy = accuracy_score(y_val, lin_predictions_binary)\nprint(f\"Linear Regression (as classifier) Validation Accuracy: {lin_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:27:13.398510Z","iopub.execute_input":"2024-07-05T02:27:13.398969Z","iopub.status.idle":"2024-07-05T02:27:13.417933Z","shell.execute_reply.started":"2024-07-05T02:27:13.398926Z","shell.execute_reply":"2024-07-05T02:27:13.416602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [0] * 418\nsubmission = pd.DataFrame({\n    'PassengerId': test_data['PassengerId'],\n    'Survived': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T02:28:15.311714Z","iopub.execute_input":"2024-07-05T02:28:15.312148Z","iopub.status.idle":"2024-07-05T02:28:15.321230Z","shell.execute_reply.started":"2024-07-05T02:28:15.312112Z","shell.execute_reply":"2024-07-05T02:28:15.319965Z"},"trusted":true},"execution_count":null,"outputs":[]}]}